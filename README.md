ğŸ€ Pipeline de AnÃ¡lisis y PredicciÃ³n NBA con Kedro
ğŸ“‹ DescripciÃ³n del Proyecto
Este proyecto analiza datos de la NBA para identificar patrones de rendimiento de equipos y genera modelos de machine learning para predecir resultados de partidos.
Utiliza el framework Kedro para crear pipelines reproducibles y modulares, integrando DVC para control de versiones de datos, Airflow para orquestaciÃ³n, y Docker para containerizaciÃ³n.

ğŸ¯ Objetivos Principales
ğŸ“Œ Identificar quÃ© equipos son mejores jugando en casa

ğŸ“Œ Analizar quÃ© equipos son peores como visitantes

ğŸ“Œ Predecir si el equipo local ganarÃ¡ (ClasificaciÃ³n)

ğŸ“Œ Predecir mÃ©tricas de rendimiento (RegresiÃ³n):

local_strength: Fuerza del equipo local

away_weakness: Debilidad del equipo visitante

point_differential: Diferencia de puntos

home_advantage: Ventaja de jugar en casa

ğŸ“Œ Crear pipelines modulares y reproducibles

ğŸ¤– Modelos de Machine Learning
ğŸ¯ Modelo de ClasificaciÃ³n
Objetivo: Predecir si el equipo local ganarÃ¡ el partido

Algoritmos implementados:

Logistic Regression

Decision Tree Classifier

Random Forest Classifier

Support Vector Machine (SVC)

Gaussian Naive Bayes

CaracterÃ­sticas principales:

Tasas de victoria histÃ³ricas (local/visitante)

Rendimiento reciente (Ãºltimos 10 partidos)

MÃ©tricas de fuerza del equipo

Historial de enfrentamientos

MÃ©tricas de evaluaciÃ³n:

PrecisiÃ³n (Accuracy)

PrecisiÃ³n (Precision)

Recall

PuntuaciÃ³n F1

ROC-AUC

ğŸ“ˆ Modelo de RegresiÃ³n
Objetivo: Predecir mÃ©tricas continuas de rendimiento

Variables objetivo:

local_strength: PuntuaciÃ³n de fuerza del equipo local (0-100)

away_weakness: Ãndice de debilidad del visitante (0-100)

point_differential: Diferencia esperada de puntos (Â±)

home_advantage: Ventaja por jugar en casa (0-20)

MÃ©tricas de evaluaciÃ³n:

RMSE (Error CuadrÃ¡tico Medio)

MAE (Error Absoluto Medio)

PuntuaciÃ³n RÂ²


IntegraciÃ³n de Clustering 

ClasificaciÃ³n: 36 nuevas features agregadas (de 17 a 53 features totales)

RegresiÃ³n: 21 nuevas features agregadas (de 17 a 38 features totales)

Clusters encontrados: 2 clusters con silhouette score de 0.6237

ğŸ“Š Rendimiento de Modelos:

ClasificaciÃ³n:

Mejor modelo: Logistic Regression

Accuracy: 99.61% âœ…

F1-Score: 99.67% âœ…

AUC-ROC: 99.89% âœ…

Mejora vs baseline: +69.7% ğŸ“ˆ

RegresiÃ³n:

Local Strength (PTS_home): RÂ² = 0.7797 (Ridge)

Away Weakness (PTS_away): RÂ² = 0.7769 (Ridge)

Point Differential: RÂ² = 0.7368 (Ridge)

ğŸ¯ Features de Clustering Agregadas:

Para ClasificaciÃ³n (36 features):

HOME_CLUSTER_0, HOME_CLUSTER_1 (one-hot encoding)

AWAY_CLUSTER_0, AWAY_CLUSTER_1 (one-hot encoding)

SAME_CLUSTER, CLUSTER_DIFF

EstadÃ­sticas por cluster para ambos equipos

Para RegresiÃ³n (21 features):

CLUSTER_0, CLUSTER_1 (one-hot encoding)

EstadÃ­sticas de cluster

Distancias a centroides

ğŸ“ˆ Impacto de la IntegraciÃ³n:

Mejor comprensiÃ³n de los patrones de equipos

Features adicionales que capturan relaciones entre equipos

Mejor rendimiento predictivo en ambos tipos de modelos

Cumplimiento completo de los requisitos de integraciÃ³n supervisado + no supervisado

ğŸš€ Inicio RÃ¡pido
Prerrequisitos
Python 3.8+

Docker y Docker Compose

Git

OpciÃ³n 1: Docker (Recomendada)

```# Clonar repositorio
git clone <url-del-repositorio>
cd nba-analysis

# Construir y ejecutar con Docker
docker-compose up --build

# Acceder a los servicios:
# Airflow: http://localhost:8080 (usuario: airflow, contraseÃ±a: airflow)
# Kedro Viz: http://localhost:4141


OpciÃ³n 2: Desarrollo Local
```
# Clonar y configurar
git clone <url-del-repositorio>
cd nba-analysis

# Crear entorno virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate  # Windows

# Instalar dependencias
pip install -r requirements.txt

# Configurar datos
mkdir -p data/01_raw
# Colocar datasets en data/01_raw/

# Inicializar DVC
dvc init

# Ejecutar pipeline
kedro run
```

Interfaz Web con Streamlit
El proyecto incluye una interfaz web interactiva desarrollada con Streamlit...

ğŸ“Š Datasets Utilizados
El proyecto utiliza 3 datasets principales de Kaggle (NBA Games):

Dataset	DescripciÃ³n
games.csv	Resultados y estadÃ­sticas de partidos
games_details.csv	EstadÃ­sticas por jugador por partido
teams.csv	InformaciÃ³n de los equipos
Fuente: Kaggle - NBA Games Dataset

ğŸ³ ConfiguraciÃ³n de Docker

Construir y Ejecutar
```
# Construir imÃ¡genes
docker-compose build

# Ejecutar todos los servicios
docker-compose up -d

# Ver logs
docker-compose logs -f

# Detener servicios
docker-compose down
```

Resumen de Servicios
Servicio	Puerto	PropÃ³sito
Pipeline Kedro	-	Procesamiento de datos y entrenamiento de modelos
Airflow	8080	OrquestaciÃ³n de pipelines
Kedro Viz	4141	VisualizaciÃ³n de pipelines
Streamlit	8501	Interfaz web interactiva para predicciones y anÃ¡lisis


Comandos Ãštiles de Docker

```

# Ejecutar comandos especÃ­ficos
docker-compose run app kedro run
docker-compose run app kedro test

# Acceder a la shell del contenedor
docker-compose exec app bash

# Ver logs de servicios especÃ­ficos
docker-compose logs -f app

```

ğŸŒªï¸ OrquestaciÃ³n con Airflow

Iniciar Airflow
```
# Usando Docker Compose
docker-compose -f docker-compose-airflow.yml up -d

# Acceder a la interfaz web: http://localhost:8080
# Usuario: airflow, ContraseÃ±a: airflow
```
DAG Principal - Pipeline NBA
El DAG de Airflow ejecuta automÃ¡ticamente:

1.Preprocesamiento de datos y validaciÃ³n

2.Entrenamiento de modelos (clasificaciÃ³n y regresiÃ³n)

3.EvaluaciÃ³n de modelos y cÃ¡lculo de mÃ©tricas

4.GeneraciÃ³n de predicciones y reportes

Comandos de Airflow
```

# Listar DAGs
docker-compose -f docker-compose-airflow.yml exec webserver airflow dags list

# Ejecutar DAG manualmente
docker-compose -f docker-compose-airflow.yml exec webserver airflow dags trigger nba_pipeline

# Ver estado de tareas
docker-compose -f docker-compose-airflow.yml exec webserver airflow tasks list nba_pipeline
```
ğŸ“Š DVC (Control de Versiones de Datos)

ConfiguraciÃ³n Inicial
```
# Inicializar DVC
dvc init

# Configurar almacenamiento remoto (ejemplo local)
dvc remote add -d myremote /ruta/al/almacenamiento

# Para almacenamiento en la nube (ejemplo AWS S3):
dvc remote add -d myremote s3://mi-bucket/nba-data

Flujo de Trabajo con DVC

# AÃ±adir datasets al seguimiento de DVC
dvc add data/01_raw/games.csv
dvc add data/01_raw/games_details.csv
dvc add data/01_raw/teams.csv

# AÃ±adir modelos al seguimiento
dvc add models/classification_model.pkl
dvc add models/regression_model.pkl

# Hacer commit en Git
git add .dvc data/01_raw/.gitignore models/.gitignore
git commit -m "AÃ±adir datasets y modelos a DVC"
```
Flujo de Trabajo Diario
```
# Obtener datos mÃ¡s recientes
dvc pull

# Ejecutar pipeline
kedro run

# AÃ±adir nuevos resultados a DVC
dvc add data/06_models/predictions.csv
dvc add models/

# Subir cambios al almacenamiento remoto
dvc push

# Hacer commit de cambios de metadatos
git add .dvc
git commit -m "Actualizar modelos y predicciones"
git push
```

Reproducir Experimentos
```
# Reproducir pipeline especÃ­fico
dvc repro pipelines/data_processing

# Verificar estado
dvc status

# Comparar mÃ©tricas entre versiones
dvc metrics diff
```
ğŸƒâ€â™‚ï¸ EjecuciÃ³n del Proyecto

Pipeline Completo
```
# Usando Kedro directamente
kedro run

# Usando Docker
docker-compose run app kedro run

# Usando Airflow
# Activar el DAG nba_pipeline en la interfaz de Airflow
```
Pipelines EspecÃ­ficos
```
# Solo procesamiento de datos
kedro run --pipeline=data_engineering


# Solo entrenamiento de clasificaciÃ³n
kedro run --pipeline=classification

# Solo entrenamiento de regresiÃ³n
kedro run --pipeline=regression
```
VisualizaciÃ³n y AnÃ¡lisis
```
# VisualizaciÃ³n del pipeline
kedro viz
# Acceder: http://localhost:4141

# Notebooks Jupyter con contexto de Kedro
kedro jupyter notebook

# Notebook de anÃ¡lisis especÃ­fico
kedro jupyter notebook --notebook-path notebooks/04_model_analysis.ipynb
```
ğŸ”§ Comandos Ãštiles
InformaciÃ³n del Proyecto
```
# Resumen del proyecto
kedro info

# Listar datasets disponibles
kedro catalog list

# Ejecutar tests
kedro test

# Crear nuevo pipeline
kedro pipeline create <nombre_pipeline>
```
GestiÃ³n de Servicios
```
# Iniciar todos los servicios
docker-compose up -d

# Detener todos los servicios
docker-compose down

# Verificar estado de servicios
docker-compose ps

# Ver logs de servicios
docker-compose logs -f <nombre_servicio>
```
GestiÃ³n de Datos y Modelos
```
# Verificar estado de DVC
dvc status

# Obtener datos mÃ¡s recientes
dvc pull

# Subir cambios al remoto
dvc push

# Reproducir pipeline
dvc repro
```
