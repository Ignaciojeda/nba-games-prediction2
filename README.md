ğŸ€ Pipeline de AnÃ¡lisis y PredicciÃ³n NBA con Kedro
ğŸ“‹ DescripciÃ³n del Proyecto
Este proyecto analiza datos de la NBA para identificar patrones de rendimiento de equipos y genera modelos de machine learning para predecir resultados de partidos.
Utiliza el framework Kedro para crear pipelines reproducibles y modulares, integrando DVC para control de versiones de datos, Airflow para orquestaciÃ³n, y Docker para containerizaciÃ³n.

ğŸ¯ Objetivos Principales
ğŸ“Œ Identificar quÃ© equipos son mejores jugando en casa

ğŸ“Œ Analizar quÃ© equipos son peores como visitantes

ğŸ“Œ Predecir si el equipo local ganarÃ¡ (ClasificaciÃ³n)

ğŸ“Œ Predecir mÃ©tricas de rendimiento (RegresiÃ³n):

local_strength: Fuerza del equipo local

away_weakness: Debilidad del equipo visitante

point_differential: Diferencia de puntos

home_advantage: Ventaja de jugar en casa

ğŸ“Œ Crear pipelines modulares y reproducibles

ğŸ¤– Modelos de Machine Learning
ğŸ¯ Modelo de ClasificaciÃ³n
Objetivo: Predecir si el equipo local ganarÃ¡ el partido

Algoritmos implementados:

Logistic Regression

Decision Tree Classifier

Random Forest Classifier

Support Vector Machine (SVC)

Gaussian Naive Bayes

CaracterÃ­sticas principales:

Tasas de victoria histÃ³ricas (local/visitante)

Rendimiento reciente (Ãºltimos 10 partidos)

MÃ©tricas de fuerza del equipo

Historial de enfrentamientos

MÃ©tricas de evaluaciÃ³n:

PrecisiÃ³n (Accuracy)

PrecisiÃ³n (Precision)

Recall

PuntuaciÃ³n F1

ROC-AUC

ğŸ“ˆ Modelo de RegresiÃ³n
Objetivo: Predecir mÃ©tricas continuas de rendimiento

Variables objetivo:

local_strength: PuntuaciÃ³n de fuerza del equipo local (0-100)

away_weakness: Ãndice de debilidad del visitante (0-100)

point_differential: Diferencia esperada de puntos (Â±)

home_advantage: Ventaja por jugar en casa (0-20)

MÃ©tricas de evaluaciÃ³n:

RMSE (Error CuadrÃ¡tico Medio)

MAE (Error Absoluto Medio)

PuntuaciÃ³n RÂ²

ğŸš€ Inicio RÃ¡pido
Prerrequisitos
Python 3.8+

Docker y Docker Compose

Git

OpciÃ³n 1: Docker (Recomendada)

# Clonar repositorio
git clone <url-del-repositorio>
cd nba-analysis

# Construir y ejecutar con Docker
docker-compose up --build

# Acceder a los servicios:
# Airflow: http://localhost:8080 (usuario: airflow, contraseÃ±a: airflow)
# Kedro Viz: http://localhost:4141

OpciÃ³n 2: Desarrollo Local
# Clonar y configurar
git clone <url-del-repositorio>
cd nba-analysis

# Crear entorno virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate  # Windows

# Instalar dependencias
pip install -r requirements.txt

# Configurar datos
mkdir -p data/01_raw
# Colocar datasets en data/01_raw/

# Inicializar DVC
dvc init

# Ejecutar pipeline
kedro run

ğŸ“Š Datasets Utilizados
El proyecto utiliza 3 datasets principales de Kaggle (NBA Games):

Dataset	DescripciÃ³n
games.csv	Resultados y estadÃ­sticas de partidos
games_details.csv	EstadÃ­sticas por jugador por partido
teams.csv	InformaciÃ³n de los equipos
Fuente: Kaggle - NBA Games Dataset

ğŸ³ ConfiguraciÃ³n de Docker
Construir y Ejecutar
# Construir imÃ¡genes
docker-compose build

# Ejecutar todos los servicios
docker-compose up -d

# Ver logs
docker-compose logs -f

# Detener servicios
docker-compose down

Resumen de Servicios
Servicio	Puerto	PropÃ³sito
Pipeline Kedro	-	Procesamiento de datos y entrenamiento de modelos
Airflow	8080	OrquestaciÃ³n de pipelines
Kedro Viz	4141	VisualizaciÃ³n de pipelines

Comandos Ãštiles de Docker

# Ejecutar comandos especÃ­ficos
docker-compose run app kedro run
docker-compose run app kedro test

# Acceder a la shell del contenedor
docker-compose exec app bash

# Ver logs de servicios especÃ­ficos
docker-compose logs -f app

ğŸŒªï¸ OrquestaciÃ³n con Airflow

Iniciar Airflow

# Usando Docker Compose
docker-compose -f docker-compose-airflow.yml up -d

# Acceder a la interfaz web: http://localhost:8080
# Usuario: airflow, ContraseÃ±a: airflow

DAG Principal - Pipeline NBA
El DAG de Airflow ejecuta automÃ¡ticamente:

1.Preprocesamiento de datos y validaciÃ³n

2.Entrenamiento de modelos (clasificaciÃ³n y regresiÃ³n)

3.EvaluaciÃ³n de modelos y cÃ¡lculo de mÃ©tricas

4.GeneraciÃ³n de predicciones y reportes

Comandos de Airflow

# Listar DAGs
docker-compose -f docker-compose-airflow.yml exec webserver airflow dags list

# Ejecutar DAG manualmente
docker-compose -f docker-compose-airflow.yml exec webserver airflow dags trigger nba_pipeline

# Ver estado de tareas
docker-compose -f docker-compose-airflow.yml exec webserver airflow tasks list nba_pipeline

ğŸ“Š DVC (Control de Versiones de Datos)

ConfiguraciÃ³n Inicial

# Inicializar DVC
dvc init

# Configurar almacenamiento remoto (ejemplo local)
dvc remote add -d myremote /ruta/al/almacenamiento

# Para almacenamiento en la nube (ejemplo AWS S3):
dvc remote add -d myremote s3://mi-bucket/nba-data

Flujo de Trabajo con DVC

# AÃ±adir datasets al seguimiento de DVC
dvc add data/01_raw/games.csv
dvc add data/01_raw/games_details.csv
dvc add data/01_raw/teams.csv

# AÃ±adir modelos al seguimiento
dvc add models/classification_model.pkl
dvc add models/regression_model.pkl

# Hacer commit en Git
git add .dvc data/01_raw/.gitignore models/.gitignore
git commit -m "AÃ±adir datasets y modelos a DVC"

Flujo de Trabajo Diario

# Obtener datos mÃ¡s recientes
dvc pull

# Ejecutar pipeline
kedro run

# AÃ±adir nuevos resultados a DVC
dvc add data/06_models/predictions.csv
dvc add models/

# Subir cambios al almacenamiento remoto
dvc push

# Hacer commit de cambios de metadatos
git add .dvc
git commit -m "Actualizar modelos y predicciones"
git push

Reproducir Experimentos

# Reproducir pipeline especÃ­fico
dvc repro pipelines/data_processing

# Verificar estado
dvc status

# Comparar mÃ©tricas entre versiones
dvc metrics diff

ğŸƒâ€â™‚ï¸ EjecuciÃ³n del Proyecto

Pipeline Completo

# Usando Kedro directamente
kedro run

# Usando Docker
docker-compose run app kedro run

# Usando Airflow
# Activar el DAG nba_pipeline en la interfaz de Airflow

Pipelines EspecÃ­ficos

# Solo procesamiento de datos
kedro run --pipeline=data_engineering


# Solo entrenamiento de clasificaciÃ³n
kedro run --pipeline=classification

# Solo entrenamiento de regresiÃ³n
kedro run --pipeline=regression

VisualizaciÃ³n y AnÃ¡lisis

# VisualizaciÃ³n del pipeline
kedro viz
# Acceder: http://localhost:4141

# Notebooks Jupyter con contexto de Kedro
kedro jupyter notebook

# Notebook de anÃ¡lisis especÃ­fico
kedro jupyter notebook --notebook-path notebooks/04_model_analysis.ipynb

ğŸ“ Estructura del Proyecto

nba-analysis/
â”œâ”€â”€ data/                   # Directorio de datos (seguido por DVC)
â”‚   â”œâ”€â”€ 01_raw/            # Datasets crudos
â”‚   â”œâ”€â”€ 02_intermediate/   # Datos procesados
â”‚   â”œâ”€â”€ 03_primary/        # Datos con ingenierÃ­a de caracterÃ­sticas
â”‚   â”œâ”€â”€ 04_feature/        # CaracterÃ­sticas para ML
â”‚   â”œâ”€â”€ 05_model_input/    # Datos listos para modelos
â”‚   â”œâ”€â”€ 06_models/         # Modelos entrenados
â”‚   â””â”€â”€ 07_model_output/   # Predicciones
â”‚   â””â”€â”€ 08_reporting/      # Reportes
â”œâ”€â”€ models/                 # Modelos serializados
â”œâ”€â”€ notebooks/              # Notebooks de anÃ¡lisis
â”‚   â”œâ”€â”€ 01_business_understanding.ipynb
â”‚   â”œâ”€â”€ 02_data_understanding.ipynb
â”‚   â”œâ”€â”€ 03_data_preparation.ipynb
â”‚   â””â”€â”€ Modelos de regresion.ipynb
â”‚   â””â”€â”€ Modelos_de_clasificacion.ipynb
â”œâ”€â”€ src/
â”‚   â””â”€â”€ nba_analysis/      # Paquete del proyecto Kedro
â”œâ”€â”€ airflow/               # DAGs y configuraciÃ³n de Airflow
â”œâ”€â”€ docker-compose.yml    # ConfiguraciÃ³n de Docker
â””â”€â”€ requirements.txt      # Dependencias de Python

ğŸ”§ Comandos Ãštiles
InformaciÃ³n del Proyecto

# Resumen del proyecto
kedro info

# Listar datasets disponibles
kedro catalog list

# Ejecutar tests
kedro test

# Crear nuevo pipeline
kedro pipeline create <nombre_pipeline>

GestiÃ³n de Servicios

# Iniciar todos los servicios
docker-compose up -d

# Detener todos los servicios
docker-compose down

# Verificar estado de servicios
docker-compose ps

# Ver logs de servicios
docker-compose logs -f <nombre_servicio>

GestiÃ³n de Datos y Modelos

# Verificar estado de DVC
dvc status

# Obtener datos mÃ¡s recientes
dvc pull

# Subir cambios al remoto
dvc push

# Reproducir pipeline
dvc repro